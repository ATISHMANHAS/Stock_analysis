{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33a98fd8-8929-4e93-95a0-f593bc2c82e2",
   "metadata": {},
   "source": [
    " Major Types of Gradient Descent\n",
    "Batch Gradient Descent:\n",
    "\n",
    "Computes the gradient of the cost function using the entire training dataset.\n",
    "Pros: Provides a stable convergence path.\n",
    "Cons: Can be slow and inefficient for large datasets.\n",
    "Stochastic Gradient Descent (SGD):\n",
    "\n",
    "Updates the model for each training example individually.\n",
    "Pros: Faster convergence and can escape local minima due to its noisy updates.\n",
    "Cons: Can lead to high variance in the loss function.\n",
    "Mini-Batch Gradient Descent:\n",
    "\n",
    "A compromise between batch and stochastic, it uses a small random subset (mini-batch) of the training data to compute the gradient.\n",
    "Pros: Balances the efficiency of batch gradient descent with the fast updates of SGD.\n",
    "Momentum:\n",
    "\n",
    "An extension of SGD that helps accelerate gradients vectors in the right directions, thus leading to faster converging.\n",
    "Pros: Reduces oscillations and speeds up convergence in relevant directions.\n",
    "Adaptive Learning Rate Methods (e.g., AdaGrad, RMSProp, Adam):\n",
    "\n",
    "Adjusts the learning rate for each parameter individually based on the historical gradient information.\n",
    "Pros: Can lead to faster convergence and can adapt to different data.\n",
    "Q2: Problems with Gradient Descent\n",
    "Choosing the Learning Rate:\n",
    "\n",
    "A learning rate that is too high can cause divergence, while a rate that is too low can result in slow convergence.\n",
    "Local Minima:\n",
    "\n",
    "The algorithm can get stuck in local minima rather than finding the global minimum.\n",
    "Saddle Points:\n",
    "\n",
    "Gradient descent can be slow to escape saddle points, which are points where the gradient is close to zero but are not minima.\n",
    "Scale Sensitivity:\n",
    "\n",
    "The performance can vary significantly depending on the scale of the input features.\n",
    "Q3: Handling Gradient Descent Problems\n",
    "Adaptive Learning Rates:\n",
    "\n",
    "Use methods like Adam or RMSProp that adjust the learning rate dynamically based on the gradient history.\n",
    "Feature Scaling:\n",
    "\n",
    "Normalize or standardize input features to ensure they are on a similar scale, which helps improve convergence speed.\n",
    "Multiple Initializations:\n",
    "\n",
    "Run the gradient descent multiple times with different starting points to mitigate the effects of local minima.\n",
    "Momentum and Nesterov Accelerated Gradient:\n",
    "\n",
    "Use momentum techniques to help smooth out the updates and escape saddle points.\n",
    "Gradient Noise:\n",
    "\n",
    "Introduce noise into the gradient calculations to help escape local minima or saddle points.\n",
    "Q4: Limitations of Ridge and Lasso Regression\n",
    "Ridge Regression:\n",
    "\n",
    "Limitations:\n",
    "Does not perform variable selection; all predictors are retained.\n",
    "Can be less interpretable when many variables are included.\n",
    "May not perform well if there are highly correlated predictors.\n",
    "Lasso Regression:\n",
    "\n",
    "Limitations:\n",
    "Can completely eliminate some variables, which may lead to a loss of potentially useful information.\n",
    "If predictors are highly correlated, Lasso may arbitrarily choose one over others, leading to instability in variable selection.\n",
    "Q5: What is the Elastic Net?\n",
    "Elastic Net is a regularization technique that combines both Lasso and Ridge regression penalties. It incorporates both the L1 (Lasso) and L2 (Ridge) penalties to the loss function, allowing for:\n",
    "Feature selection (like Lasso) by forcing some coefficients to be exactly zero.\n",
    "Group effect (like Ridge), where correlated variables can be included in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd229162-de19-437f-a5e2-628c66ebd04b",
   "metadata": {},
   "source": [
    "Q1. Major Types of Gradient Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bc999a-7eb3-4fec-a98a-75ae7b7d5840",
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch Gradient Descent:\n",
    "\n",
    "Computes the gradient of the cost function using the entire training dataset.\n",
    "Pros: Provides a stable convergence path.\n",
    "Cons: Can be slow and inefficient for large datasets.\n",
    "Stochastic Gradient Descent (SGD):\n",
    "\n",
    "Updates the model for each training example individually.\n",
    "Pros: Faster convergence and can escape local minima due to its noisy updates.\n",
    "Cons: Can lead to high variance in the loss function.\n",
    "Mini-Batch Gradient Descent:\n",
    "\n",
    "A compromise between batch and stochastic, it uses a small random subset (mini-batch) of the training data to compute the gradient.\n",
    "Pros: Balances the efficiency of batch gradient descent with the fast updates of SGD.\n",
    "Momentum:\n",
    "\n",
    "An extension of SGD that helps accelerate gradients vectors in the right directions, thus leading to faster converging.\n",
    "Pros: Reduces oscillations and speeds up convergence in relevant directions.\n",
    "Adaptive Learning Rate Methods (e.g., AdaGrad, RMSProp, Adam):\n",
    "\n",
    "Adjusts the learning rate for each parameter individually based on the historical gradient information.\n",
    "Pros: Can lead to faster convergence and can adapt to different data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea62ebf-74e8-4b79-8a9d-9fc543c2bd5d",
   "metadata": {},
   "source": [
    "Q2: Problems with Gradient Descent"
   ]
  },
  {
   "cell_type": "raw",
   "id": "60393298-2a13-487f-8dfe-3f555c99b14a",
   "metadata": {},
   "source": [
    "Choosing the Learning Rate:\n",
    "\n",
    "A learning rate that is too high can cause divergence, while a rate that is too low can result in slow convergence.\n",
    "Local Minima:\n",
    "\n",
    "The algorithm can get stuck in local minima rather than finding the global minimum.\n",
    "Saddle Points:\n",
    "\n",
    "Gradient descent can be slow to escape saddle points, which are points where the gradient is close to zero but are not minima.\n",
    "Scale Sensitivity:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242c7006-9dea-45f5-8332-749b2212a5a4",
   "metadata": {},
   "source": [
    "Q3: Handling Gradient Descent Problems"
   ]
  },
  {
   "cell_type": "raw",
   "id": "82c8de6a-db61-49bb-91ef-04a65ce8029d",
   "metadata": {},
   "source": [
    "Adaptive Learning Rates:\n",
    "\n",
    "Use methods like Adam or RMSProp that adjust the learning rate dynamically based on the gradient history.\n",
    "Feature Scaling:\n",
    "\n",
    "Normalize or standardize input features to ensure they are on a similar scale, which helps improve convergence speed.\n",
    "Multiple Initializations:\n",
    "\n",
    "Run the gradient descent multiple times with different starting points to mitigate the effects of local minima.\n",
    "Momentum and Nesterov Accelerated Gradient:\n",
    "\n",
    "Use momentum techniques to help smooth out the updates and escape saddle points.\n",
    "Gradient Noise:\n",
    "\n",
    "Introduce noise into the gradient calculations to help escape local minima or saddle points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd78661-2db1-4de6-92c8-288ce9114f82",
   "metadata": {},
   "source": [
    "Q4: Limitations of Ridge and Lasso Regression"
   ]
  },
  {
   "cell_type": "raw",
   "id": "913e31b8-2ca8-4201-9b78-664169fa697c",
   "metadata": {},
   "source": [
    "Does not perform variable selection; all predictors are retained.\n",
    "Can be less interpretable when many variables are included.\n",
    "May not perform well if there are highly correlated predictors.\n",
    "Lasso Regression:\n",
    "\n",
    "Limitations:\n",
    "Can completely eliminate some variables, which may lead to a loss of potentially useful information.\n",
    "If predictors are highly correlated, Lasso may arbitrarily choose one over others, leading to instability in variable selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0f016d-3f40-42a1-8b40-815d0ef1de15",
   "metadata": {},
   "source": [
    "Q5: What is the Elastic Net?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "882d90ba-6360-4425-b266-a20e228119a8",
   "metadata": {},
   "source": [
    "Elastic Net is a regularization technique that combines both Lasso and Ridge regression penalties. It incorporates both the L1 (Lasso) and L2 (Ridge) penalties to the loss function, allowing for:\n",
    "Feature selection (like Lasso) by forcing some coefficients to be exactly zero.\n",
    "Group effect (like Ridge), where correlated variables can be included in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7c6584-fdf4-4f7e-9dca-7d88775b54e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
